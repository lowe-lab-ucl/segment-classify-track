{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1cada0f4",
   "metadata": {},
   "source": [
    "# Phenotype classifcation using CellX \n",
    "\n",
    "This notebook shows how to take segmented time lapse microscopy images and use h2b fluorescence markers to classfiy mitotic state of the cell cycle. \n",
    "\n",
    "The sections of this notebook are as follows:\n",
    "\n",
    "1. Load images\n",
    "2. Localise the objects\n",
    "3. Classify the objects\n",
    "4. Batch process\n",
    "\n",
    "The data used in this notebook is timelapse microscopy data with h2b-gfp/rfp markers that show the spatial extent of the nucleus and it's mitotic state. \n",
    "\n",
    "This notebook uses the dask octopuslite image loader from the CellX/Lowe lab project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c311e3db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from octopuslite import DaskOctopusLiteLoader\n",
    "import btrack\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from scipy.special import softmax\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.io import imread, imshow\n",
    "from cellx import load_model\n",
    "from cellx.tools.image import InfinitePaddedImage\n",
    "from skimage.transform import resize\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = [18,8]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a5906f",
   "metadata": {},
   "source": [
    "## 1. Load segmentation images\n",
    "\n",
    "#### *Important:* from this point on you will need to be consistent with the use of cropping and alignment. \n",
    "Using a previously generated alignment transformation will aid greatly in the tracking notebook, which depends on the object localisation performed in this notebook. Cropping your images will ensure that no border effects from the translational shift are seen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67ee9d2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cropping: (1200, 1600)\n"
     ]
    }
   ],
   "source": [
    "# load images\n",
    "expt = 'ND0011'\n",
    "pos = 'Pos6'\n",
    "root_dir = '/home/nathan/data'\n",
    "image_path = f'{root_dir}/{expt}/{pos}/{pos}_images'\n",
    "transform_path = f'{root_dir}/{expt}/{pos}/gfp_transform_tensor.npy'\n",
    "images = DaskOctopusLiteLoader(image_path, \n",
    "                               transforms=transform_path,\n",
    "                               crop=(1200,1600), \n",
    "                               remove_background=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfccac2e",
   "metadata": {},
   "source": [
    "## 2. Localise the objects\n",
    "We need to also measure the mean intensity regionprops parameter in order to differentiate object class, for which we need to provide an image to measure. This means we need to provide the segmentation images twice: once to find the centroid and once to measure the pixel intensity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1fbeab93",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO][2022/01/31 04:10:55 PM] Localizing objects from segmentation...\n",
      "[INFO][2022/01/31 04:10:55 PM] Found intensity_image data\n",
      "[INFO][2022/01/31 04:10:55 PM] Calculating weighted centroids using intensity_image\n",
      "[INFO][2022/01/31 04:14:24 PM] Objects are of type: <class 'dict'>\n",
      "[INFO][2022/01/31 04:14:24 PM] ...Found 6933 objects in 1638 frames.\n"
     ]
    }
   ],
   "source": [
    "objects = btrack.utils.segmentation_to_objects(\n",
    "    images['mask'], images['mask'],\n",
    "    properties = ('area', 'max_intensity', ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "11fbbc77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "      <th>t</th>\n",
       "      <th>dummy</th>\n",
       "      <th>states</th>\n",
       "      <th>label</th>\n",
       "      <th>prob</th>\n",
       "      <th>area</th>\n",
       "      <th>max_intensity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6931</td>\n",
       "      <td>803.494489</td>\n",
       "      <td>620.610806</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1637</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>198320</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "{'ID': 6931, 'x': 803.494488705123, 'y': 620.6108057684551, 'z': 0.0, 't': 1637, 'dummy': False, 'states': 0, 'label': 5, 'prob': 0.0, 'area': 198320, 'max_intensity': 1}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "objects[6931]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa49b7b",
   "metadata": {},
   "source": [
    "#### Can also assign measured values from raw image to each segment using `skimage.measure.regionprops` parameters\n",
    "But also need to load the raw images to be measured first. Cannot currently save out `intensity_image` parameter to object file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4548905",
   "metadata": {},
   "outputs": [],
   "source": [
    "detailed_objects = btrack.utils.segmentation_to_objects(\n",
    "    images['mask'], \n",
    "    images['gfp'],\n",
    "    properties = ('area', 'mean_intensity', 'intensity_image'), \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e2efcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "detailed_objects[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63775a4c",
   "metadata": {},
   "source": [
    "Example image showing PCNA-iRFP morphology "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d3a75b",
   "metadata": {},
   "outputs": [],
   "source": [
    "imshow(detailed_objects[0].properties['intensity_image'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08114355",
   "metadata": {},
   "source": [
    "## 2b. Differentiate the objects based on class ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "583c2fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "objects_gfp = [obj for obg in objects if obj.properties['max_intensity'] == 1]\n",
    "objects_rfp = [obj for obg in objects if obj.properties['max_intensity'] == 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc05b98",
   "metadata": {},
   "source": [
    "## 3. Classify the objects "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1725583",
   "metadata": {},
   "source": [
    "Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "10cd85e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('./models/cellx_classifier_stardist.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bce619f",
   "metadata": {},
   "source": [
    "Define normalisation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "92d135ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_channels(x):\n",
    "\n",
    "    for dim in range(x.shape[-1]):\n",
    "        x[..., dim] = normalize(x[..., dim])\n",
    "        \n",
    "    return x\n",
    "\n",
    "def normalize(x):\n",
    "\n",
    "    xf = x.astype(np.float32)\n",
    "    mx = np.mean(xf)\n",
    "    sd = np.max([np.std(xf), 1./np.prod(x.shape)])\n",
    "\n",
    "    return (xf - mx) / sd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb8428f",
   "metadata": {},
   "source": [
    "Define classifier function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "41184a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_objects(image, objects, obj_type):\n",
    "    \n",
    "    # define stages of cell cycle to classify (dependent on model type)\n",
    "    LABELS = [\"interphase\", \"prometaphase\", \"metaphase\", \"anaphase\", \"apoptosis\"]\n",
    "    \n",
    "    # iterate over frames\n",
    "    for n in tqdm(range(image.shape[0])):\n",
    "        \n",
    "        # only select objects if in frame\n",
    "        _objects = [o for o in objects if o.t == n]\n",
    "        \n",
    "        # empty placeholder arrays\n",
    "        crops = []\n",
    "        to_update = []\n",
    "        \n",
    "        # select h2b channel to aid in classification\n",
    "        fp = gfp if obj_type == 1 else rfp\n",
    "        \n",
    "        # create stack by computing each frame of dask array input\n",
    "        frame = np.stack(\n",
    "            [image[n, ...].compute(), fp[n, ...].compute()], \n",
    "            axis=-1,) \n",
    "        \n",
    "        # create padded image for network\n",
    "        vol = InfinitePaddedImage(frame, mode = 'reflect')\n",
    "        \n",
    "        # iterate over objects \n",
    "        for obj in _objects:\n",
    "            \n",
    "            # create coords for image slice\n",
    "            xs = slice(int(obj.x-40), int(obj.x+40), 1)\n",
    "            ys = slice(int(obj.y-40), int(obj.y+40), 1)\n",
    "            \n",
    "            # crop image\n",
    "            crop = vol[ys, xs, :]\n",
    "            crop = resize(crop, (64, 64), preserve_range=True).astype(np.float32)\n",
    "            \n",
    "            # normalise image\n",
    "            if crop.shape == (64 ,64, 2):\n",
    "                crops.append(normalize_channels(crop))\n",
    "                to_update.append(obj)\n",
    "            else:\n",
    "                print(crop.shape)\n",
    "                \n",
    "        if not crops:\n",
    "            continue\n",
    "            \n",
    "        # use classifcation model to predict\n",
    "        pred = model.predict(np.stack(crops, axis=0))\n",
    "        \n",
    "        # check correct number of predictions\n",
    "        assert pred.shape[0] == len(_objects)\n",
    "        \n",
    "        # assign labels to objects\n",
    "        for idx in range(pred.shape[0]):\n",
    "            obj = _objects[idx]\n",
    "            \n",
    "            # assigning details of prediction\n",
    "            pred_label = np.argmax(pred[idx, ...])\n",
    "            pred_softmax = softmax(pred[idx, ...])\n",
    "            logits = {f\"prob_{k}\": pred_softmax[ki] for ki, k in enumerate(LABELS)}\n",
    "            \n",
    "            # write out\n",
    "            obj.label = pred_label\n",
    "            obj.properties = logits\n",
    "\n",
    "    return objects"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8781b4",
   "metadata": {},
   "source": [
    "#### Load raw images for classifier, a colour channel dependent on `obj_type` needed too (i.e. GFP is `obj_type = 1`, RFP is `obj_type = 2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "657771d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "bf = images['brightfield']\n",
    "gfp = images['gfp']\n",
    "rfp = images['rfp']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30835a86",
   "metadata": {},
   "source": [
    "#### Classify objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420e84c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "objects_gfp = classify_objects(bf, objects_gfp, obj_type = 1)\n",
    "objects_rfp = classify_objects(bf, objects_rfp, obj_type = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c70646be",
   "metadata": {},
   "source": [
    "#### Inspect an example object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec4f611",
   "metadata": {},
   "outputs": [],
   "source": [
    "objects_gfp[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c5c716",
   "metadata": {},
   "source": [
    "#### Save out classified GFP objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "796afd43",
   "metadata": {},
   "outputs": [],
   "source": [
    "with btrack.dataio.HDF5FileHandler(\n",
    "    f'{root_dir}/{expt}/{pos}/objects.h5', 'w', obj_type='obj_type_1',\n",
    ") as hdf:\n",
    "    #hdf.write_segmentation(masks['mask'])\n",
    "    hdf.write_objects(objects_gfp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fdb0274",
   "metadata": {},
   "source": [
    "#### Save out classified RFP objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71de0f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "with btrack.dataio.HDF5FileHandler(\n",
    "    f'{root_dir}/{expt}/{pos}/objects.h5', 'w', obj_type='obj_type_2',\n",
    ") as hdf:\n",
    "    #hdf.write_segmentation(masks['mask'])\n",
    "    hdf.write_objects(objects_rfp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b2e10e7",
   "metadata": {},
   "source": [
    "# 4. Batch process\n",
    "Iterate over many experiments and positions (need to ensure you define normalisation and classification functions above first)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1792133",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = '/home/nathan/data'\n",
    "expt_list = ['ND0009', 'ND0010', 'ND0011']\n",
    "pos_list = 'all'\n",
    "overwrite = False\n",
    "\n",
    "for expt in expt_list:\n",
    "    \n",
    "        # Find all positions in that experiment, if pos_list is all then it finds all positions\n",
    "        if pos_list == 'all':\n",
    "            pos_list = [pos for pos in os.listdir(f'{root_dir}/{expt}') \n",
    "                    if 'Pos' in pos \n",
    "                    and os.path.isdir(f'{root_dir}/{expt}/{pos}')]  \n",
    "            \n",
    "        ### Iterate over all positions in that experiment\n",
    "        for pos in pos_list:\n",
    "            \n",
    "            ### check if overwrite param is false check if raw directory already created and if type of transform file already exists and decide whether to skip pos\n",
    "            if not overwrite and glob.glob(f'{root_dir}/{expt}/{pos}/*objects*.h5'):\n",
    "                print(glob.glob(f'{root_dir}/{expt}/{pos}/*objects*.h5'), f'file found, skipping {expt}/{pos}')\n",
    "                continue\n",
    "                \n",
    "            print(f'Starting {expt}/{pos}')\n",
    "            # load segmentation images and apply necessary transforms and crops\n",
    "            image_path = f'{root_dir}/{expt}/{pos}/{pos}_images'\n",
    "            transform_path = f'{root_dir}/{expt}/{pos}/gfp_transform_tensor.npy'\n",
    "            images = DaskOctopusLiteLoader(image_path, \n",
    "                               transforms=transform_path,\n",
    "                               crop=(1200,1600), \n",
    "                               remove_background=False)\n",
    "            \n",
    "            # ID the objects in each segmentation image and assign option properties to them\n",
    "            objects = btrack.utils.segmentation_to_objects(\n",
    "                images['mask'], images['mask'],\n",
    "                properties = ('area', 'max_intensity', ),\n",
    "            )\n",
    "            \n",
    "            # differentiate the objects based on class ID\n",
    "            objects_gfp = [obj for obg in objects if obj.properties['max_intensity'] == 1]\n",
    "            objects_rfp = [obj for obg in objects if obj.properties['max_intensity'] == 2]\n",
    "            \n",
    "            # load classifcation model and define labels\n",
    "            model = load_model('./models/cellx_classifier_stardist.h5')\n",
    "            LABELS = [\"interphase\", \"prometaphase\", \"metaphase\", \"anaphase\", \"apoptosis\"]\n",
    "\n",
    "            # load images for classifcation\n",
    "            bf = images['brightfield']\n",
    "            gfp = images['gfp']\n",
    "            rfp = images['rfp']\n",
    "            \n",
    "            # classify objects\n",
    "            print(\"Classifying objects\")\n",
    "            objects_gfp = classify_objects(bf, objects_gfp, obj_type = 1)\n",
    "            objects_rfp = classify_objects(bf, objects_rfp, obj_type = 2)\n",
    "            \n",
    "            # save out classified objects as segmentation h5 file\n",
    "            with btrack.dataio.HDF5FileHandler(\n",
    "                f'{root_dir}/{expt}/{pos}/objects.h5', 'w', obj_type='obj_type_1',\n",
    "            ) as hdf:\n",
    "                #hdf.write_segmentation(masks['mask'])\n",
    "                hdf.write_objects(objects_gfp)\n",
    "            with btrack.dataio.HDF5FileHandler(\n",
    "                f'{root_dir}/{expt}/{pos}/objects.h5', 'w', obj_type='obj_type_2',\n",
    "            ) as hdf:\n",
    "                #hdf.write_segmentation(masks['mask'])\n",
    "                hdf.write_objects(objects_rfp)     "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CellX",
   "language": "python",
   "name": "cellx"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
