{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1cada0f4",
   "metadata": {},
   "source": [
    "# Phenotype classifcation using CellX \n",
    "\n",
    "This notebook shows how to take segmented time lapse microscopy images and use h2b fluorescence markers to classfiy mitotic state of the cell cycle. \n",
    "\n",
    "The sections of this notebook are as follows:\n",
    "\n",
    "1. Load images\n",
    "2. Localise the objects\n",
    "3. Classify the objects\n",
    "4. Filter the objects\n",
    "5. Run btrack, uniting the objects locations over time\n",
    "\n",
    "The data used in this notebook is timelapse microscopy data with h2b-gfp/rfp markers that show the spatial extent of the nucleus and it's mitotic state. \n",
    "\n",
    "This notebook uses the dask octopuslite image loader from the CellX/Lowe lab project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c311e3db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from octopuslite import DaskOctopusLiteLoader, image_generator\n",
    "import btrack\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from scipy.special import softmax\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.io import imread, imshow\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = [18,8]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a5906f",
   "metadata": {},
   "source": [
    "## 1. Load segmentation images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "67ee9d2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cropping: (1200, 1600)\n"
     ]
    }
   ],
   "source": [
    "# load images\n",
    "expt = 'ND0011'\n",
    "pos = 'Pos6'\n",
    "root_dir = '/home/nathan/data'\n",
    "image_path = f'{root_dir}/{expt}/{pos}/{pos}_images'\n",
    "images = DaskOctopusLiteLoader(image_path, crop=(1200,1600), remove_background=False)\n",
    "## efficiently load segmentation images by providing the path to the generator function\n",
    "segmentation = image_generator(images.files('mask'), crop=(1200,1600))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfccac2e",
   "metadata": {},
   "source": [
    "## 2. Localise the objects\n",
    "We need to also measure the mean intensity regionprops parameter in order to differentiate object class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e130c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "objects = btrack.utils.segmentation_to_objects(\n",
    "    segmentation,\n",
    "    properties = ('area', 'mean_intensity'),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa49b7b",
   "metadata": {},
   "source": [
    "#### Can also assign measured values from raw image to each segment using `skimage.measure.regionprops` parameters\n",
    "But also need to load the raw images to be measured first and redefine the segmentation image generator as we are using again. Cannot currently save out `intensity_image` parameter to object file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a4548905",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cropping: (1200, 1600)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO][2022/01/21 04:44:05 PM] Localizing objects from segmentation...\n",
      "[INFO][2022/01/21 04:44:05 PM] Found intensity_image data\n",
      "[INFO][2022/01/21 04:44:05 PM] Calculating weighted centroids using intensity_image\n",
      "[INFO][2022/01/21 04:44:05 PM] Objects are of type: <class 'dict'>\n",
      "[INFO][2022/01/21 04:44:05 PM] ...Found 596 objects in 9 frames.\n"
     ]
    }
   ],
   "source": [
    "gfp = image_generator(images.files('gfp'), crop=(1200,1600))\n",
    "segmentation = image_generator(images.files('mask'), crop=(1200,1600))\n",
    "detailed_objects = btrack.utils.segmentation_to_objects(\n",
    "    segmentation,\n",
    "    gfp,\n",
    "    properties = ('area', 'mean_intensity', 'intensity_image'), \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "91e2efcf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "      <th>t</th>\n",
       "      <th>dummy</th>\n",
       "      <th>states</th>\n",
       "      <th>label</th>\n",
       "      <th>prob</th>\n",
       "      <th>area</th>\n",
       "      <th>intensity_image</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>284.482239</td>\n",
       "      <td>783.94415</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1026</td>\n",
       "      <td>(37, 36) array</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "{'ID': 0, 'x': 284.48223906409874, 'y': 783.9441501643441, 'z': 0.0, 't': 0, 'dummy': False, 'states': 0, 'label': 5, 'prob': 0.0, 'area': 1026, 'intensity_image': array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detailed_objects[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63775a4c",
   "metadata": {},
   "source": [
    "Example image showing PCNA-iRFP morphology "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d3a75b",
   "metadata": {},
   "outputs": [],
   "source": [
    "imshow(detailed_objects[0].properties['intensity_image'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08114355",
   "metadata": {},
   "source": [
    "## 2b. Differentiate the objects based on class ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "583c2fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "objects_gfp = [obj for obg in objects if obj.properties['mean_intensity'] == 1]\n",
    "objects_rfp = [obj for obg in objects if obj.properties['mean_intensity'] == 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc05b98",
   "metadata": {},
   "source": [
    "## 3. Classify the objects "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "10cd85e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cellx import load_model\n",
    "from cellx.tools.image import InfinitePaddedImage\n",
    "from skimage.transform import resize\n",
    "\n",
    "model = load_model('./models/cellx_classifier_stardist.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "92d135ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_channels(x):\n",
    "\n",
    "    for dim in range(x.shape[-1]):\n",
    "        x[..., dim] = normalize(x[..., dim])\n",
    "        \n",
    "    return x\n",
    "\n",
    "def normalize(x):\n",
    "\n",
    "    xf = x.astype(np.float32)\n",
    "    mx = np.mean(xf)\n",
    "    sd = np.max([np.std(xf), 1./np.prod(x.shape)])\n",
    "\n",
    "    return (xf - mx) / sd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "41184a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_objects(image, objects, obj_type=1):\n",
    "    labels = []\n",
    "    for n in tqdm(range(image.shape[0])):\n",
    "\n",
    "        _objects = [o for o in objects if o.t == n]\n",
    "\n",
    "        crops = []\n",
    "        to_update = []\n",
    "        \n",
    "        \n",
    "        fp = gfp if obj_type == 1 else rfp\n",
    "\n",
    "\n",
    "        frame = np.stack(\n",
    "            [image[n, ...].compute(), fp[n, ...].compute()], \n",
    "            axis=-1,\n",
    "        ) \n",
    "\n",
    "        vol = InfinitePaddedImage(frame, mode = 'reflect')\n",
    "\n",
    "        for obj in _objects:\n",
    "            xs = slice(int(obj.x-40), int(obj.x+40), 1)\n",
    "            ys = slice(int(obj.y-40), int(obj.y+40), 1)\n",
    "\n",
    "            crop = vol[ys, xs, :]\n",
    "            crop = resize(crop, (64, 64), preserve_range=True).astype(np.float32)\n",
    "\n",
    "            if crop.shape == (64 ,64, 2):\n",
    "                crops.append(normalize_channels(crop))\n",
    "                to_update.append(obj)\n",
    "            else:\n",
    "                print(crop.shape)\n",
    "\n",
    "        if not crops:\n",
    "            continue\n",
    "\n",
    "\n",
    "        pred = model.predict(np.stack(crops, axis=0))\n",
    "\n",
    "        assert pred.shape[0] == len(_objects)\n",
    "        for idx in range(pred.shape[0]):\n",
    "            obj = _objects[idx]\n",
    "\n",
    "            pred_label = np.argmax(pred[idx, ...])\n",
    "            pred_softmax = softmax(pred[idx, ...])\n",
    "\n",
    "            logits = {f\"prob_{k}\": pred_softmax[ki] for ki, k in enumerate(LABELS)}\n",
    "\n",
    "            obj.label = pred_label\n",
    "            obj.properties = logits\n",
    "            \n",
    "    return objects"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8781b4",
   "metadata": {},
   "source": [
    "#### Load raw images for classifier, a colour channel dependent on `obj_type` needed too (i.e. GFP is `obj_type = 1`, RFP is `obj_type = 2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "657771d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "bf = images['brightfield']\n",
    "gfp = images['gfp']\n",
    "rfp = images['rfp']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30835a86",
   "metadata": {},
   "source": [
    "#### Classify objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420e84c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "objects_gfp = classify_objects(bf, objects_gfp, obj_type = 1)\n",
    "objects_rfp = classify_objects(bf, objects_rfp, obj_type = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c70646be",
   "metadata": {},
   "source": [
    "#### Inspect an example object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec4f611",
   "metadata": {},
   "outputs": [],
   "source": [
    "objects_gfp[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c5c716",
   "metadata": {},
   "source": [
    "#### Save out classified GFP objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "796afd43",
   "metadata": {},
   "outputs": [],
   "source": [
    "with btrack.dataio.HDF5FileHandler(\n",
    "    f'{root_dir}/{expt}/{pos}/segmented.h5', 'w', obj_type='obj_type_1',\n",
    ") as hdf:\n",
    "    #hdf.write_segmentation(masks['mask'])\n",
    "    hdf.write_objects(objects_gfp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fdb0274",
   "metadata": {},
   "source": [
    "#### Save out classified RFP objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71de0f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "with btrack.dataio.HDF5FileHandler(\n",
    "    f'{root_dir}/{expt}/{pos}/segmented.h5', 'w', obj_type='obj_type_2',\n",
    ") as hdf:\n",
    "    #hdf.write_segmentation(masks['mask'])\n",
    "    hdf.write_objects(objects_rfp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b1696a",
   "metadata": {},
   "source": [
    "## 4. Filter the objects \n",
    "\n",
    "Based on segments that are too small to feasibly be cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f660dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "objects_gfp = [o for o in objects_gfp if o.properties['area']>100.]\n",
    "objects_rfp = [o for o in objects_rfp if o.properties['area']>100.]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e64ac4ae",
   "metadata": {},
   "source": [
    "## 5. Run btrack  \n",
    "\n",
    "Unite each object with it's subsequent position at the following time point and export as a tracking file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e456e142",
   "metadata": {},
   "source": [
    "#### For GFP objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb6623e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# initialise a tracker session using a context manager\n",
    "with btrack.BayesianTracker() as tracker:\n",
    "\n",
    "    # configure the tracker using a config file\n",
    "    tracker.configure_from_file(\n",
    "        './models/MDCK_config_wildtype.json'\n",
    "    )\n",
    "    tracker.max_search_radius = 40\n",
    "\n",
    "    # append the objects to be tracked\n",
    "    tracker.append(objects_gfp)\n",
    "\n",
    "    # set the volume\n",
    "    tracker.volume=((0, 1200), (0, 1600), (-1e5, 1e5))\n",
    "\n",
    "    # track them (in interactive mode)\n",
    "    tracker.track_interactive(step_size=100)\n",
    "\n",
    "    # generate hypotheses and run the global optimizer\n",
    "    tracker.optimize()\n",
    "\n",
    "    tracker.export(f'{root_dir}/{expt}/{pos}/tracks.h5', obj_type='obj_type_1')\n",
    "\n",
    "    # get the tracks in a format for napari visualization (optional)\n",
    "    visaulise_tracks, properties, graph = tracker.to_napari(ndim=2)\n",
    "    \n",
    "    gfp_tracks = tracker.tracks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e418844",
   "metadata": {},
   "source": [
    "#### For RFP objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a76bb469",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialise a tracker session using a context manager\n",
    "with btrack.BayesianTracker() as tracker:\n",
    "\n",
    "    # configure the tracker using a config file\n",
    "    tracker.configure_from_file(\n",
    "        './models/MDCK_config_wildtype.json'\n",
    "    )\n",
    "    tracker.max_search_radius = 40\n",
    "\n",
    "    # append the objects to be tracked\n",
    "    tracker.append(objects_rfp)\n",
    "\n",
    "    # set the volume\n",
    "    tracker.volume=((0, 1200), (0, 1600), (-1e5, 1e5))\n",
    "\n",
    "    # track them (in interactive mode)\n",
    "    tracker.track_interactive(step_size=100)\n",
    "\n",
    "    # generate hypotheses and run the global optimizer\n",
    "    tracker.optimize()\n",
    "\n",
    "    tracker.export(f'{root_dir}/{expt}/{pos}/tracks.h5', obj_type='obj_type_2')\n",
    "\n",
    "    # get the tracks in a format for napari visualization (optional)\n",
    "    visaulise_tracks, properties, graph = tracker.to_napari(ndim=2)\n",
    "    \n",
    "    rfp_tracks = tracker.tracks"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CellX",
   "language": "python",
   "name": "cellx"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
