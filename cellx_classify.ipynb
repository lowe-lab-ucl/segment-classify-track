{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1cada0f4",
   "metadata": {},
   "source": [
    "# Phenotype classifcation using CellX \n",
    "\n",
    "This notebook shows how to take segmented time lapse microscopy images and use h2b fluorescence markers to classfiy mitotic state of the cell cycle. \n",
    "\n",
    "The sections of this notebook are as follows:\n",
    "\n",
    "1. Load images\n",
    "2. Localise the objects\n",
    "3. Classify the objects\n",
    "4. Batch process\n",
    "\n",
    "The data used in this notebook is timelapse microscopy data with h2b-gfp/rfp markers that show the spatial extent of the nucleus and it's mitotic state. \n",
    "\n",
    "This notebook uses the dask octopuslite image loader from the CellX/Lowe lab project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c311e3db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from octopuslite import DaskOctopusLiteLoader\n",
    "import btrack\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from scipy.special import softmax\n",
    "import os\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.io import imread, imshow\n",
    "from cellx import load_model\n",
    "from cellx.tools.image import InfinitePaddedImage\n",
    "from skimage.transform import resize\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = [18,8]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a5906f",
   "metadata": {},
   "source": [
    "## 1. Load segmentation images\n",
    "\n",
    "#### *Important*: from this point on you will need to be consistent with the use of cropping and alignment.\n",
    "Using a previously generated alignment transformation will aid greatly in the tracking notebook, which depends on the object localisation performed in this notebook. Cropping your images will ensure that no border effects from the translational shift are seen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67ee9d2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cropping: (1200, 1600)\n"
     ]
    }
   ],
   "source": [
    "# load images\n",
    "expt = 'ND0011'\n",
    "pos = 'Pos6'\n",
    "root_dir = '/home/nathan/data'\n",
    "image_path = f'{root_dir}/{expt}/{pos}/{pos}_images'\n",
    "transform_path = f'{root_dir}/{expt}/{pos}/gfp_transform_tensor.npy'\n",
    "images = DaskOctopusLiteLoader(image_path, \n",
    "                               transforms=transform_path,\n",
    "                               crop=(1200,1600), \n",
    "                               remove_background=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfccac2e",
   "metadata": {},
   "source": [
    "## 2. Localise the objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "06e130c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO][2022/01/27 05:01:26 PM] Localizing objects from segmentation...\n",
      "[INFO][2022/01/27 05:03:06 PM] Objects are of type: <class 'dict'>\n",
      "[INFO][2022/01/27 05:03:06 PM] ...Found 6933 objects in 1638 frames.\n"
     ]
    }
   ],
   "source": [
    "objects = btrack.utils.segmentation_to_objects(\n",
    "    images['mask'],\n",
    "    properties = ('area', ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8277bb68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "      <th>t</th>\n",
       "      <th>dummy</th>\n",
       "      <th>states</th>\n",
       "      <th>label</th>\n",
       "      <th>prob</th>\n",
       "      <th>area</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1410.627687</td>\n",
       "      <td>558.702494</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1163</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "{'ID': 0, 'x': 1410.627687016337, 'y': 558.7024935511608, 'z': 0.0, 't': 0, 'dummy': False, 'states': 0, 'label': 5, 'prob': 0.0, 'area': 1163}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "objects[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ce32898f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(objects)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa49b7b",
   "metadata": {},
   "source": [
    "#### Can also assign measured values from raw image to each segment using `skimage.measure.regionprops` parameters\n",
    "But also need to load the raw images to be measured first. Cannot currently save out `intensity_image` parameter to object file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "a4548905",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO][2022/01/25 04:47:33 PM] Localizing objects from segmentation...\n",
      "[INFO][2022/01/25 04:47:33 PM] Found intensity_image data\n",
      "[INFO][2022/01/25 04:47:33 PM] Calculating weighted centroids using intensity_image\n",
      "[INFO][2022/01/25 04:52:07 PM] Objects are of type: <class 'dict'>\n",
      "[INFO][2022/01/25 04:52:10 PM] ...Found 327022 objects in 1638 frames.\n"
     ]
    }
   ],
   "source": [
    "detailed_objects = btrack.utils.segmentation_to_objects(\n",
    "    images['mask'], \n",
    "    images['gfp'],\n",
    "    properties = ('area', 'mean_intensity', 'intensity_image'), \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "91e2efcf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "      <th>t</th>\n",
       "      <th>dummy</th>\n",
       "      <th>states</th>\n",
       "      <th>label</th>\n",
       "      <th>prob</th>\n",
       "      <th>area</th>\n",
       "      <th>mean_intensity</th>\n",
       "      <th>intensity_image</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>69</td>\n",
       "      <td>1517.610711</td>\n",
       "      <td>906.027849</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1231</td>\n",
       "      <td>97.688871</td>\n",
       "      <td>(43, 35) array</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "{'ID': 69, 'x': 1517.610710573365, 'y': 906.0278491538813, 'z': 0.0, 't': 1, 'dummy': False, 'states': 0, 'label': 5, 'prob': 0.0, 'area': 1231, 'mean_intensity': 97.68887083671811, 'intensity_image': array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)}"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detailed_objects[69]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "03d3a75b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fb780373c70>"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1296x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# example image showing PCNA-iRFP morphology \n",
    "imshow(detailed_objects[69].properties['intensity_image'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc05b98",
   "metadata": {},
   "source": [
    "## 3. Classify the objects "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f4cae5d",
   "metadata": {},
   "source": [
    "Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "10cd85e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('./models/cellx_classifier_stardist.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b1f87b",
   "metadata": {},
   "source": [
    "Define normalisation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "92d135ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_channels(x):\n",
    "\n",
    "    for dim in range(x.shape[-1]):\n",
    "        x[..., dim] = normalize(x[..., dim])\n",
    "        \n",
    "    return x\n",
    "\n",
    "def normalize(x):\n",
    "\n",
    "    xf = x.astype(np.float32)\n",
    "    mx = np.mean(xf)\n",
    "    sd = np.max([np.std(xf), 1./np.prod(x.shape)])\n",
    "\n",
    "    return (xf - mx) / sd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb0676ab",
   "metadata": {},
   "source": [
    "Define classifier function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "41184a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_objects(image, objects, obj_type=1):\n",
    "    \n",
    "    # define stages of cell cycle to classify (dependent on model type)\n",
    "    LABELS = [\"interphase\", \"prometaphase\", \"metaphase\", \"anaphase\", \"apoptosis\"]\n",
    "    \n",
    "    # iterate over frames\n",
    "    for n in tqdm(range(image.shape[0])):\n",
    "        \n",
    "        # only select objects if in frame\n",
    "        _objects = [o for o in objects if o.t == n]\n",
    "        \n",
    "        # empty placeholder arrays\n",
    "        crops = []\n",
    "        to_update = []\n",
    "        \n",
    "        # select h2b channel to aid in classification\n",
    "        fp = gfp if obj_type == 1 else rfp\n",
    "        \n",
    "        # create stack by computing each frame of dask array input\n",
    "        frame = np.stack(\n",
    "            [image[n, ...].compute(), fp[n, ...].compute()], \n",
    "            axis=-1,) \n",
    "        \n",
    "        # create padded image for network\n",
    "        vol = InfinitePaddedImage(frame, mode = 'reflect')\n",
    "        \n",
    "        # iterate over objects \n",
    "        for obj in _objects:\n",
    "            \n",
    "            # create coords for image slice\n",
    "            xs = slice(int(obj.x-40), int(obj.x+40), 1)\n",
    "            ys = slice(int(obj.y-40), int(obj.y+40), 1)\n",
    "            \n",
    "            # crop image\n",
    "            crop = vol[ys, xs, :]\n",
    "            crop = resize(crop, (64, 64), preserve_range=True).astype(np.float32)\n",
    "            \n",
    "            # normalise image\n",
    "            if crop.shape == (64 ,64, 2):\n",
    "                crops.append(normalize_channels(crop))\n",
    "                to_update.append(obj)\n",
    "            else:\n",
    "                print(crop.shape)\n",
    "                \n",
    "        if not crops:\n",
    "            continue\n",
    "            \n",
    "        # use classifcation model to predict\n",
    "        pred = model.predict(np.stack(crops, axis=0))\n",
    "        \n",
    "        # check correct number of predictions\n",
    "        assert pred.shape[0] == len(_objects)\n",
    "        \n",
    "        # assign labels to objects\n",
    "        for idx in range(pred.shape[0]):\n",
    "            obj = _objects[idx]\n",
    "            \n",
    "            # assigning details of prediction\n",
    "            pred_label = np.argmax(pred[idx, ...])\n",
    "            pred_softmax = softmax(pred[idx, ...])\n",
    "            logits = {f\"prob_{k}\": pred_softmax[ki] for ki, k in enumerate(LABELS)}\n",
    "            \n",
    "            # write out\n",
    "            obj.label = pred_label\n",
    "            obj.properties = logits\n",
    "\n",
    "    return objects"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8781b4",
   "metadata": {},
   "source": [
    "#### Load raw images for classifier, a colour channel dependent on obj_type needed too (i.e. GFP is obj_type = 1, RFP is obj_type = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "657771d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "bf = images['brightfield']\n",
    "gfp = images['gfp']\n",
    "rfp = images['rfp']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30835a86",
   "metadata": {},
   "source": [
    "#### Classify objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "420e84c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1638/1638 [03:05<00:00,  8.82it/s]\n"
     ]
    }
   ],
   "source": [
    "objects = classify_objects(bf, objects, obj_type = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c70646be",
   "metadata": {},
   "source": [
    "#### Inspect an example object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "aec4f611",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "      <th>t</th>\n",
       "      <th>dummy</th>\n",
       "      <th>states</th>\n",
       "      <th>label</th>\n",
       "      <th>prob</th>\n",
       "      <th>area</th>\n",
       "      <th>prob_interphase</th>\n",
       "      <th>prob_prometaphase</th>\n",
       "      <th>prob_metaphase</th>\n",
       "      <th>prob_anaphase</th>\n",
       "      <th>prob_apoptosis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1410.627687</td>\n",
       "      <td>558.702494</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1163</td>\n",
       "      <td>0.999992</td>\n",
       "      <td>3.588410e-09</td>\n",
       "      <td>6.128119e-10</td>\n",
       "      <td>1.724182e-10</td>\n",
       "      <td>0.000008</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "{'ID': 0, 'x': 1410.627687016337, 'y': 558.7024935511608, 'z': 0.0, 't': 0, 'dummy': False, 'states': 0, 'label': 0, 'prob': 0.0, 'area': 1163, 'prob_interphase': 0.9999923, 'prob_prometaphase': 3.58841e-09, 'prob_metaphase': 6.1281186e-10, 'prob_anaphase': 1.7241819e-10, 'prob_apoptosis': 7.640319e-06}"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "objects[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c5c716",
   "metadata": {},
   "source": [
    "#### Save out classified objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "796afd43",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO][2022/01/25 05:17:30 PM] Opening HDF file: /home/nathan/data/ND0011/Pos6/segmented.h5...\n",
      "[INFO][2022/01/25 05:20:21 PM] Writing objects/obj_type_1\n",
      "[INFO][2022/01/25 05:20:22 PM] Writing labels/obj_type_1\n",
      "[INFO][2022/01/25 05:20:22 PM] Loading objects/obj_type_1 (327022, 5) (327022 filtered: None)\n",
      "[INFO][2022/01/25 05:20:25 PM] Writing properties/obj_type_1/area (327022,)\n",
      "[INFO][2022/01/25 05:20:25 PM] Writing properties/obj_type_1/prob_interphase (327022,)\n",
      "[INFO][2022/01/25 05:20:25 PM] Writing properties/obj_type_1/prob_prometaphase (327022,)\n",
      "[INFO][2022/01/25 05:20:25 PM] Writing properties/obj_type_1/prob_metaphase (327022,)\n",
      "[INFO][2022/01/25 05:20:25 PM] Writing properties/obj_type_1/prob_anaphase (327022,)\n",
      "[INFO][2022/01/25 05:20:26 PM] Writing properties/obj_type_1/prob_apoptosis (327022,)\n",
      "[INFO][2022/01/25 05:20:26 PM] Closing HDF file: /home/nathan/data/ND0011/Pos6/segmented.h5\n"
     ]
    }
   ],
   "source": [
    "with btrack.dataio.HDF5FileHandler(\n",
    "     f'{root_dir}/{expt}/{pos}/objects.h5', 'w', obj_type='obj_type_1',\n",
    ") as hdf:\n",
    "    hdf.write_segmentation(images['mask'])\n",
    "    hdf.write_objects(objects)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b2e10e7",
   "metadata": {},
   "source": [
    "# 4. Batch process\n",
    "Iterate over many experiments and positions (need to ensure you define normalisation and classification functions above first)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae9809c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = '/home/nathan/data'\n",
    "expt_list = ['ND0009', 'ND0010', 'ND0011']\n",
    "pos_list = 'all'\n",
    "overwrite = False\n",
    "\n",
    "for expt in expt_list:\n",
    "    \n",
    "        # Find all positions in that experiment, if pos_list is all then it finds all positions\n",
    "        if pos_list == 'all':\n",
    "            pos_list = [pos for pos in os.listdir(f'{root_dir}/{expt}') \n",
    "                    if 'Pos' in pos \n",
    "                    and os.path.isdir(f'{root_dir}/{expt}/{pos}')]  \n",
    "            \n",
    "        ### Iterate over all positions in that experiment\n",
    "        for pos in pos_list:\n",
    "            \n",
    "            ### check if overwrite param is false check if raw directory already created and if type of transform file already exists and decide whether to skip pos\n",
    "            if not overwrite and glob.glob(f'{root_dir}/{expt}/{pos}/*objects*.h5'):\n",
    "                print(glob.glob(f'{root_dir}/{expt}/{pos}/*objects*.h5'), f'file found, skipping {expt}/{pos}')\n",
    "                continue\n",
    "                \n",
    "            print(f'Starting {expt}/{pos}')\n",
    "            # load segmentation images in efficient image generator style\n",
    "            image_path = f'{root_dir}/{expt}/{pos}/{pos}_images'\n",
    "            transform_path = f'{root_dir}/{expt}/{pos}/gfp_transform_tensor.npy'\n",
    "            images = DaskOctopusLiteLoader(image_path, \n",
    "                                           transforms=transform_path,\n",
    "                                           crop=(1200,1600), \n",
    "                                           remove_background=False)\n",
    "            \n",
    "            # ID the objects in each segmentation image and assign option properties to them\n",
    "            objects = btrack.utils.segmentation_to_objects(\n",
    "            images['mask'],\n",
    "            properties = ('area', ),\n",
    "            )\n",
    "            \n",
    "            # load classifcation model and define labels\n",
    "            model = load_model('./models/cellx_classifier_stardist.h5')\n",
    "            LABELS = [\"interphase\", \"prometaphase\", \"metaphase\", \"anaphase\", \"apoptosis\"]\n",
    "\n",
    "            # load images for classifcation\n",
    "            bf = images['brightfield']\n",
    "            gfp = images['gfp'] # or rfp = images['rfp'], dependent on cell type\n",
    "            \n",
    "            # classify objects\n",
    "            print(\"Classifying objects\")\n",
    "            objects = classify_objects(bf, objects, obj_type = 1)\n",
    "            \n",
    "            # save out classified objects as segmentation h5 file\n",
    "            with btrack.dataio.HDF5FileHandler(\n",
    "                 f'{root_dir}/{expt}/{pos}/objectss.h5', 'w', obj_type='obj_type_1',\n",
    "            ) as hdf:\n",
    "                hdf.write_segmentation(images['mask'])\n",
    "                hdf.write_objects(objects)\n",
    "            \n",
    "            print(f'Finished {expt}/{pos}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CellX",
   "language": "python",
   "name": "cellx"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
