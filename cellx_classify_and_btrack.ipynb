{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1cada0f4",
   "metadata": {},
   "source": [
    "# Phenotype classifcation using CellX \n",
    "\n",
    "This notebook shows how to take segmented time lapse microscopy images and use h2b fluorescence markers to classfiy mitotic state of the cell cycle. \n",
    "\n",
    "The sections of this notebook are as follows:\n",
    "\n",
    "1. Load images\n",
    "2. Localise the objects\n",
    "3. Classify the objects\n",
    "4. Filter the objects\n",
    "5. Run btrack, uniting the objects locations over time\n",
    "\n",
    "The data used in this notebook is timelapse microscopy data with h2b-gfp/rfp markers that show the spatial extent of the nucleus and it's mitotic state. \n",
    "\n",
    "This notebook uses the dask octopuslite image loader from the CellX/Lowe lab project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c311e3db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from octopuslite import DaskOctopusLiteLoader, image_generator\n",
    "import btrack\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from scipy.special import softmax\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.io import imread, imshow\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = [18,8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "907a6f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_generator(files, crop = None):\n",
    "    \"\"\"Image generator for iterative procesess\"\"\"\n",
    "    #get dims\n",
    "    shape = imread(files[0]).shape\n",
    "    dims = imread(files[0]).ndim\n",
    "    if crop == None:\n",
    "        for filename in files:\n",
    "            img = imread(filename)\n",
    "            yield img\n",
    "    else:\n",
    "        cslice = lambda d: slice(\n",
    "            int((shape[d] - crop[d]) // 2),\n",
    "            int((shape[d] - crop[d]) // 2 + crop[d]))\n",
    "        crops = tuple([cslice(d) for d in range(dims)])\n",
    "        for filename in files:\n",
    "            img = imread(filename)[crops]\n",
    "            yield img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a5906f",
   "metadata": {},
   "source": [
    "## 1. Load segmentation images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "67ee9d2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cropping: (1200, 1600)\n"
     ]
    }
   ],
   "source": [
    "# load images\n",
    "expt = 'ND0011'\n",
    "pos = 'Pos6'\n",
    "root_dir = '/home/nathan/data'\n",
    "image_path = f'{root_dir}/{expt}/{pos}/{pos}_images'\n",
    "images = DaskOctopusLiteLoader(image_path, crop=(1200,1600), remove_background=False)\n",
    "## efficiently load segmentation images by providing the path to the generator function\n",
    "segmentation = image_generator(images.files('mask'), crop=(1200,1600))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfccac2e",
   "metadata": {},
   "source": [
    "## 2. Localise the objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "06e130c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO][2022/01/25 04:52:11 PM] Localizing objects from segmentation...\n",
      "[INFO][2022/01/25 04:53:22 PM] Objects are of type: <class 'dict'>\n",
      "[INFO][2022/01/25 04:53:24 PM] ...Found 327022 objects in 1638 frames.\n"
     ]
    }
   ],
   "source": [
    "objects = btrack.utils.segmentation_to_objects(\n",
    "    segmentation,\n",
    "    properties = ('area', ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "edd32afd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "      <th>t</th>\n",
       "      <th>dummy</th>\n",
       "      <th>states</th>\n",
       "      <th>label</th>\n",
       "      <th>prob</th>\n",
       "      <th>area</th>\n",
       "      <th>prob_interphase</th>\n",
       "      <th>prob_prometaphase</th>\n",
       "      <th>prob_metaphase</th>\n",
       "      <th>prob_anaphase</th>\n",
       "      <th>prob_apoptosis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1410.627687</td>\n",
       "      <td>558.702494</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1163</td>\n",
       "      <td>0.999992</td>\n",
       "      <td>3.588410e-09</td>\n",
       "      <td>6.128119e-10</td>\n",
       "      <td>1.724182e-10</td>\n",
       "      <td>0.000008</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "{'ID': 0, 'x': 1410.627687016337, 'y': 558.7024935511608, 'z': 0.0, 't': 0, 'dummy': False, 'states': 0, 'label': 0, 'prob': 0.0, 'area': 1163, 'prob_interphase': 0.9999923, 'prob_prometaphase': 3.58841e-09, 'prob_metaphase': 6.1281186e-10, 'prob_anaphase': 1.7241819e-10, 'prob_apoptosis': 7.640319e-06}"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "objects[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa49b7b",
   "metadata": {},
   "source": [
    "#### Can also assign measured values from raw image to each segment using `skimage.measure.regionprops` parameters\n",
    "But also need to load the raw images to be measured first and redefine the segmentation image generator as we are using again. Cannot currently save out `intensity_image` parameter to object file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "a4548905",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO][2022/01/25 04:47:33 PM] Localizing objects from segmentation...\n",
      "[INFO][2022/01/25 04:47:33 PM] Found intensity_image data\n",
      "[INFO][2022/01/25 04:47:33 PM] Calculating weighted centroids using intensity_image\n",
      "[INFO][2022/01/25 04:52:07 PM] Objects are of type: <class 'dict'>\n",
      "[INFO][2022/01/25 04:52:10 PM] ...Found 327022 objects in 1638 frames.\n"
     ]
    }
   ],
   "source": [
    "irfp = image_generator(images.files('irfp'), crop=(1200,1600))\n",
    "segmentation = image_generator(images.files('mask'), crop=(1200,1600))\n",
    "detailed_objects = btrack.utils.segmentation_to_objects(\n",
    "    segmentation,\n",
    "    irfp,\n",
    "    properties = ('area', 'mean_intensity', 'intensity_image'), \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "91e2efcf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "      <th>t</th>\n",
       "      <th>dummy</th>\n",
       "      <th>states</th>\n",
       "      <th>label</th>\n",
       "      <th>prob</th>\n",
       "      <th>area</th>\n",
       "      <th>mean_intensity</th>\n",
       "      <th>intensity_image</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>69</td>\n",
       "      <td>1517.610711</td>\n",
       "      <td>906.027849</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1231</td>\n",
       "      <td>97.688871</td>\n",
       "      <td>(43, 35) array</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "{'ID': 69, 'x': 1517.610710573365, 'y': 906.0278491538813, 'z': 0.0, 't': 1, 'dummy': False, 'states': 0, 'label': 5, 'prob': 0.0, 'area': 1231, 'mean_intensity': 97.68887083671811, 'intensity_image': array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)}"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detailed_objects[69]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "03d3a75b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fb780373c70>"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1296x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# example image showing PCNA-iRFP morphology \n",
    "imshow(detailed_objects[69].properties['intensity_image'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc05b98",
   "metadata": {},
   "source": [
    "## 3. Classify the objects "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "10cd85e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cellx import load_model\n",
    "from cellx.tools.image import InfinitePaddedImage\n",
    "from skimage.transform import resize\n",
    "\n",
    "model = load_model('./models/cellx_classifier_stardist.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4bd5b60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "LABELS = [\"interphase\", \"prometaphase\", \"metaphase\", \"anaphase\", \"apoptosis\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "92d135ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_channels(x):\n",
    "\n",
    "    for dim in range(x.shape[-1]):\n",
    "        x[..., dim] = normalize(x[..., dim])\n",
    "        \n",
    "    return x\n",
    "\n",
    "def normalize(x):\n",
    "\n",
    "    xf = x.astype(np.float32)\n",
    "    mx = np.mean(xf)\n",
    "    sd = np.max([np.std(xf), 1./np.prod(x.shape)])\n",
    "\n",
    "    return (xf - mx) / sd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "41184a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_objects(image, objects, obj_type=1):\n",
    "    labels = []\n",
    "    for n in tqdm(range(image.shape[0])):\n",
    "\n",
    "        _objects = [o for o in objects if o.t == n]\n",
    "\n",
    "        crops = []\n",
    "        to_update = []\n",
    "        \n",
    "        \n",
    "        fp = gfp if obj_type == 1 else rfp\n",
    "\n",
    "\n",
    "        frame = np.stack(\n",
    "            [image[n, ...].compute(), fp[n, ...].compute()], \n",
    "            axis=-1,\n",
    "        ) \n",
    "\n",
    "        vol = InfinitePaddedImage(frame, mode = 'reflect')\n",
    "\n",
    "        for obj in _objects:\n",
    "            xs = slice(int(obj.x-40), int(obj.x+40), 1)\n",
    "            ys = slice(int(obj.y-40), int(obj.y+40), 1)\n",
    "\n",
    "            crop = vol[ys, xs, :]\n",
    "            crop = resize(crop, (64, 64), preserve_range=True).astype(np.float32)\n",
    "\n",
    "            if crop.shape == (64 ,64, 2):\n",
    "                crops.append(normalize_channels(crop))\n",
    "                to_update.append(obj)\n",
    "            else:\n",
    "                print(crop.shape)\n",
    "\n",
    "        if not crops:\n",
    "            continue\n",
    "\n",
    "\n",
    "        pred = model.predict(np.stack(crops, axis=0))\n",
    "\n",
    "        assert pred.shape[0] == len(_objects)\n",
    "        for idx in range(pred.shape[0]):\n",
    "            obj = _objects[idx]\n",
    "\n",
    "            pred_label = np.argmax(pred[idx, ...])\n",
    "            pred_softmax = softmax(pred[idx, ...])\n",
    "\n",
    "            logits = {f\"prob_{k}\": pred_softmax[ki] for ki, k in enumerate(LABELS)}\n",
    "\n",
    "            obj.label = pred_label\n",
    "            obj.properties = logits\n",
    "            \n",
    "    return objects"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8781b4",
   "metadata": {},
   "source": [
    "#### Load raw brightfield images for classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "657771d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "bf = images['brightfield']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30835a86",
   "metadata": {},
   "source": [
    "#### Classify objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "420e84c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1638/1638 [22:37<00:00,  1.21it/s]\n"
     ]
    }
   ],
   "source": [
    "objects = classify_objects(bf, objects, obj_type = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c70646be",
   "metadata": {},
   "source": [
    "#### Inspect an example object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "aec4f611",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "      <th>t</th>\n",
       "      <th>dummy</th>\n",
       "      <th>states</th>\n",
       "      <th>label</th>\n",
       "      <th>prob</th>\n",
       "      <th>area</th>\n",
       "      <th>prob_interphase</th>\n",
       "      <th>prob_prometaphase</th>\n",
       "      <th>prob_metaphase</th>\n",
       "      <th>prob_anaphase</th>\n",
       "      <th>prob_apoptosis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1410.627687</td>\n",
       "      <td>558.702494</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1163</td>\n",
       "      <td>0.999992</td>\n",
       "      <td>3.588410e-09</td>\n",
       "      <td>6.128119e-10</td>\n",
       "      <td>1.724182e-10</td>\n",
       "      <td>0.000008</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "{'ID': 0, 'x': 1410.627687016337, 'y': 558.7024935511608, 'z': 0.0, 't': 0, 'dummy': False, 'states': 0, 'label': 0, 'prob': 0.0, 'area': 1163, 'prob_interphase': 0.9999923, 'prob_prometaphase': 3.58841e-09, 'prob_metaphase': 6.1281186e-10, 'prob_anaphase': 1.7241819e-10, 'prob_apoptosis': 7.640319e-06}"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "objects[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c5c716",
   "metadata": {},
   "source": [
    "#### Save out classified objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "796afd43",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO][2022/01/25 05:17:30 PM] Opening HDF file: /home/nathan/data/ND0011/Pos6/segmented.h5...\n",
      "[INFO][2022/01/25 05:20:21 PM] Writing objects/obj_type_1\n",
      "[INFO][2022/01/25 05:20:22 PM] Writing labels/obj_type_1\n",
      "[INFO][2022/01/25 05:20:22 PM] Loading objects/obj_type_1 (327022, 5) (327022 filtered: None)\n",
      "[INFO][2022/01/25 05:20:25 PM] Writing properties/obj_type_1/area (327022,)\n",
      "[INFO][2022/01/25 05:20:25 PM] Writing properties/obj_type_1/prob_interphase (327022,)\n",
      "[INFO][2022/01/25 05:20:25 PM] Writing properties/obj_type_1/prob_prometaphase (327022,)\n",
      "[INFO][2022/01/25 05:20:25 PM] Writing properties/obj_type_1/prob_metaphase (327022,)\n",
      "[INFO][2022/01/25 05:20:25 PM] Writing properties/obj_type_1/prob_anaphase (327022,)\n",
      "[INFO][2022/01/25 05:20:26 PM] Writing properties/obj_type_1/prob_apoptosis (327022,)\n",
      "[INFO][2022/01/25 05:20:26 PM] Closing HDF file: /home/nathan/data/ND0011/Pos6/segmented.h5\n"
     ]
    }
   ],
   "source": [
    "with btrack.dataio.HDF5FileHandler(\n",
    "     f'{root_dir}/{expt}/{pos}/segmented.h5'), 'w', obj_type='obj_type_1',\n",
    ") as hdf:\n",
    "    hdf.write_segmentation(images['mask'])\n",
    "    hdf.write_objects(objects)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b1696a",
   "metadata": {},
   "source": [
    "## 4. Filter the objects \n",
    "\n",
    "Excluding segments that are too small to feasibly be cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "62f660dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "objects = [o for o in objects if o.properties['area']>100.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "6d097d6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['MDCK_config_new.json',\n",
       " 'MDCK_config_wildtype.json',\n",
       " 'stardist_multiclass_MDCK',\n",
       " 'stardist_singleclass_MDCK',\n",
       " 'MDCK_config_wildtype_dense.json',\n",
       " 'cellx_classifier_stardist.h5',\n",
       " 'json_example.json',\n",
       " 'README.md',\n",
       " 'MDCK_config_scribble_sparse.json']"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir('./models/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e64ac4ae",
   "metadata": {},
   "source": [
    "## 5. Run btrack  \n",
    "\n",
    "Unite each object with it's subsequent position at the following time point and export as a tracking file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "bdb6623e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO][2022/01/25 06:03:14 PM] Loaded btrack: /home/nathan/analysis/BayesianTracker/btrack/libs/libtracker.so\n",
      "[INFO][2022/01/25 06:03:14 PM] btrack (v0.4.3) library imported\n",
      "[INFO][2022/01/25 06:03:14 PM] Setting max XYZ search radius to: 100\n",
      "[INFO][2022/01/25 06:03:14 PM] Starting BayesianTracker session\n",
      "[INFO][2022/01/25 06:03:14 PM] Loading configuration file: ./models/MDCK_config_wildtype.json\n",
      "[INFO][2022/01/25 06:03:14 PM] Loading motion model: b'MDCK_motion'\n",
      "[INFO][2022/01/25 06:03:14 PM] Setting max XYZ search radius to: 40\n",
      "[INFO][2022/01/25 06:03:14 PM] Objects are of type: <class 'list'>\n",
      "[INFO][2022/01/25 06:03:15 PM] Set volume to ((0, 1200), (0, 1600), (-100000.0, 100000.0))\n",
      "[INFO][2022/01/25 06:03:15 PM] Starting tracking... \n",
      "[INFO][2022/01/25 06:03:15 PM] Tracking objects in frames 0 to 99 (of 1638)...\n",
      "[INFO][2022/01/25 06:03:15 PM]  - Timing (Bayesian updates: 0.52ms, Linking: 0.16ms)\n",
      "[INFO][2022/01/25 06:03:15 PM]  - Probabilities (Link: 0.99997, Lost: 0.92068)\n",
      "[INFO][2022/01/25 06:03:15 PM]  - Stats (Active: 49, Lost: 946, Conflicts resolved: 581)\n",
      "[INFO][2022/01/25 06:03:15 PM] Tracking objects in frames 100 to 199 (of 1638)...\n",
      "[INFO][2022/01/25 06:03:15 PM]  - Timing (Bayesian updates: 0.66ms, Linking: 0.18ms)\n",
      "[INFO][2022/01/25 06:03:15 PM]  - Probabilities (Link: 0.99051, Lost: 1.00000)\n",
      "[INFO][2022/01/25 06:03:15 PM]  - Stats (Active: 52, Lost: 1401, Conflicts resolved: 786)\n",
      "[INFO][2022/01/25 06:03:15 PM] Tracking objects in frames 200 to 299 (of 1638)...\n",
      "[INFO][2022/01/25 06:03:15 PM]  - Timing (Bayesian updates: 0.94ms, Linking: 0.22ms)\n",
      "[INFO][2022/01/25 06:03:15 PM]  - Probabilities (Link: 1.00000, Lost: 0.69318)\n",
      "[INFO][2022/01/25 06:03:15 PM]  - Stats (Active: 65, Lost: 1716, Conflicts resolved: 954)\n",
      "[INFO][2022/01/25 06:03:15 PM] Tracking objects in frames 300 to 399 (of 1638)...\n",
      "[INFO][2022/01/25 06:03:16 PM]  - Timing (Bayesian updates: 1.08ms, Linking: 0.26ms)\n",
      "[INFO][2022/01/25 06:03:16 PM]  - Probabilities (Link: 0.99598, Lost: 0.48261)\n",
      "[INFO][2022/01/25 06:03:16 PM]  - Stats (Active: 66, Lost: 2308, Conflicts resolved: 1315)\n",
      "[INFO][2022/01/25 06:03:16 PM] Tracking objects in frames 400 to 499 (of 1638)...\n",
      "[INFO][2022/01/25 06:03:16 PM]  - Timing (Bayesian updates: 1.61ms, Linking: 0.25ms)\n",
      "[INFO][2022/01/25 06:03:16 PM]  - Probabilities (Link: 0.99999, Lost: 1.00000)\n",
      "[INFO][2022/01/25 06:03:16 PM]  - Stats (Active: 79, Lost: 3204, Conflicts resolved: 1907)\n",
      "[INFO][2022/01/25 06:03:16 PM] Tracking objects in frames 500 to 599 (of 1638)...\n",
      "[INFO][2022/01/25 06:03:16 PM]  - Timing (Bayesian updates: 3.15ms, Linking: 0.38ms)\n",
      "[INFO][2022/01/25 06:03:16 PM]  - Probabilities (Link: 1.00000, Lost: 0.16302)\n",
      "[INFO][2022/01/25 06:03:16 PM]  - Stats (Active: 109, Lost: 3932, Conflicts resolved: 2273)\n",
      "[INFO][2022/01/25 06:03:16 PM] Tracking objects in frames 600 to 699 (of 1638)...\n",
      "[INFO][2022/01/25 06:03:17 PM]  - Timing (Bayesian updates: 3.49ms, Linking: 0.40ms)\n",
      "[INFO][2022/01/25 06:03:17 PM]  - Probabilities (Link: 1.00000, Lost: 0.75622)\n",
      "[INFO][2022/01/25 06:03:17 PM]  - Stats (Active: 112, Lost: 4632, Conflicts resolved: 2608)\n",
      "[INFO][2022/01/25 06:03:17 PM] Tracking objects in frames 700 to 799 (of 1638)...\n",
      "[INFO][2022/01/25 06:03:18 PM]  - Timing (Bayesian updates: 7.13ms, Linking: 0.55ms)\n",
      "[INFO][2022/01/25 06:03:18 PM]  - Probabilities (Link: 1.00000, Lost: 1.00000)\n",
      "[INFO][2022/01/25 06:03:18 PM]  - Stats (Active: 167, Lost: 6190, Conflicts resolved: 3591)\n",
      "[INFO][2022/01/25 06:03:18 PM] Tracking objects in frames 800 to 899 (of 1638)...\n",
      "[INFO][2022/01/25 06:03:19 PM]  - Timing (Bayesian updates: 8.12ms, Linking: 0.60ms)\n",
      "[INFO][2022/01/25 06:03:19 PM]  - Probabilities (Link: 1.00000, Lost: 1.00000)\n",
      "[INFO][2022/01/25 06:03:19 PM]  - Stats (Active: 170, Lost: 7415, Conflicts resolved: 4124)\n",
      "[INFO][2022/01/25 06:03:19 PM] Tracking objects in frames 900 to 999 (of 1638)...\n",
      "[INFO][2022/01/25 06:03:20 PM]  - Timing (Bayesian updates: 11.88ms, Linking: 0.69ms)\n",
      "[INFO][2022/01/25 06:03:20 PM]  - Probabilities (Link: 1.00000, Lost: 1.00000)\n",
      "[INFO][2022/01/25 06:03:20 PM]  - Stats (Active: 203, Lost: 9211, Conflicts resolved: 5198)\n",
      "[INFO][2022/01/25 06:03:20 PM] Tracking objects in frames 1000 to 1099 (of 1638)...\n",
      "[INFO][2022/01/25 06:03:22 PM]  - Timing (Bayesian updates: 15.10ms, Linking: 0.91ms)\n",
      "[INFO][2022/01/25 06:03:22 PM]  - Probabilities (Link: 0.99972, Lost: 1.00000)\n",
      "[INFO][2022/01/25 06:03:22 PM]  - Stats (Active: 218, Lost: 11066, Conflicts resolved: 6277)\n",
      "[INFO][2022/01/25 06:03:22 PM] Tracking objects in frames 1100 to 1199 (of 1638)...\n",
      "[INFO][2022/01/25 06:03:25 PM]  - Timing (Bayesian updates: 17.94ms, Linking: 0.81ms)\n",
      "[INFO][2022/01/25 06:03:25 PM]  - Probabilities (Link: 0.99999, Lost: 1.00000)\n",
      "[INFO][2022/01/25 06:03:25 PM]  - Stats (Active: 231, Lost: 12842, Conflicts resolved: 7331)\n",
      "[INFO][2022/01/25 06:03:25 PM] Tracking objects in frames 1200 to 1299 (of 1638)...\n",
      "[INFO][2022/01/25 06:03:27 PM]  - Timing (Bayesian updates: 19.67ms, Linking: 1.01ms)\n",
      "[INFO][2022/01/25 06:03:27 PM]  - Probabilities (Link: 0.99868, Lost: 0.99474)\n",
      "[INFO][2022/01/25 06:03:27 PM]  - Stats (Active: 239, Lost: 15015, Conflicts resolved: 8840)\n",
      "[INFO][2022/01/25 06:03:27 PM] Tracking objects in frames 1300 to 1399 (of 1638)...\n",
      "[INFO][2022/01/25 06:03:30 PM]  - Timing (Bayesian updates: 20.48ms, Linking: 0.90ms)\n",
      "[INFO][2022/01/25 06:03:30 PM]  - Probabilities (Link: 0.99999, Lost: 0.40925)\n",
      "[INFO][2022/01/25 06:03:30 PM]  - Stats (Active: 243, Lost: 16309, Conflicts resolved: 9588)\n",
      "[INFO][2022/01/25 06:03:30 PM] Tracking objects in frames 1400 to 1499 (of 1638)...\n",
      "[INFO][2022/01/25 06:03:34 PM]  - Timing (Bayesian updates: 25.06ms, Linking: 1.03ms)\n",
      "[INFO][2022/01/25 06:03:34 PM]  - Probabilities (Link: 1.00000, Lost: 0.70622)\n",
      "[INFO][2022/01/25 06:03:34 PM]  - Stats (Active: 268, Lost: 17828, Conflicts resolved: 10566)\n",
      "[INFO][2022/01/25 06:03:34 PM] Tracking objects in frames 1500 to 1599 (of 1638)...\n",
      "[INFO][2022/01/25 06:03:38 PM]  - Timing (Bayesian updates: 30.42ms, Linking: 1.15ms)\n",
      "[INFO][2022/01/25 06:03:38 PM]  - Probabilities (Link: 1.00000, Lost: 1.00000)\n",
      "[INFO][2022/01/25 06:03:38 PM]  - Stats (Active: 308, Lost: 21093, Conflicts resolved: 12707)\n",
      "[INFO][2022/01/25 06:03:38 PM] Tracking objects in frames 1600 to 1638 (of 1638)...\n",
      "[INFO][2022/01/25 06:03:39 PM]  - Timing (Bayesian updates: 27.98ms, Linking: 1.12ms)\n",
      "[INFO][2022/01/25 06:03:39 PM]  - Probabilities (Link: 1.00000, Lost: 1.00000)\n",
      "[INFO][2022/01/25 06:03:39 PM] SUCCESS.\n",
      "[INFO][2022/01/25 06:03:39 PM]  - Found 97405 tracks in 1638 frames (in 0.0s)\n",
      "[INFO][2022/01/25 06:03:40 PM]  - Inserted 4148 dummy objects to fill tracking gaps\n",
      "[INFO][2022/01/25 06:03:40 PM] Loading hypothesis model: MDCK_hypothesis_wildtype\n",
      "[INFO][2022/01/25 06:03:40 PM] Calculating hypotheses (relax: True)...\n",
      "[INFO][2022/01/25 06:03:42 PM] Setting up constraints matrix for global optimisation...\n",
      "[INFO][2022/01/25 06:09:02 PM] Optimizing...\n",
      "[INFO][2022/01/25 06:10:33 PM] Optimization complete. (Solution: optimal)\n",
      "[INFO][2022/01/25 06:10:34 PM]  - Fates.FALSE_POSITIVE: 1365 (of 97405)\n",
      "[INFO][2022/01/25 06:10:34 PM]  - Fates.LINK: 2722 (of 235480)\n",
      "[INFO][2022/01/25 06:10:34 PM]  - Fates.DIVIDE: 275 (of 229815)\n",
      "[INFO][2022/01/25 06:10:34 PM]  - Fates.APOPTOSIS: 169 (of 257)\n",
      "[INFO][2022/01/25 06:10:34 PM]  - Fates.INITIALIZE_BORDER: 91661 (of 91943)\n",
      "[INFO][2022/01/25 06:10:34 PM]  - Fates.INITIALIZE_FRONT: 45 (of 50)\n",
      "[INFO][2022/01/25 06:10:34 PM]  - Fates.INITIALIZE_LAZY: 1062 (of 5412)\n",
      "[INFO][2022/01/25 06:10:34 PM]  - Fates.TERMINATE_BORDER: 91654 (of 91932)\n",
      "[INFO][2022/01/25 06:10:34 PM]  - Fates.TERMINATE_BACK: 244 (of 281)\n",
      "[INFO][2022/01/25 06:10:34 PM]  - Fates.TERMINATE_LAZY: 976 (of 5192)\n",
      "[INFO][2022/01/25 06:10:34 PM]  - TOTAL: 757767 hypotheses\n",
      "[INFO][2022/01/25 06:10:34 PM] Completed optimization with 94683 tracks\n",
      "[INFO][2022/01/25 06:10:35 PM] Opening HDF file: /home/nathan/data/ND0011/Pos6/tracks.h5...\n",
      "[INFO][2022/01/25 06:10:40 PM] Writing tracks/obj_type_1\n",
      "[WARNING][2022/01/25 06:10:40 PM] Removing tracks/obj_type_1.\n",
      "[INFO][2022/01/25 06:10:40 PM] Writing dummies/obj_type_1\n",
      "[INFO][2022/01/25 06:10:40 PM] Writing LBEP/obj_type_1\n",
      "[INFO][2022/01/25 06:10:40 PM] Writing fates/obj_type_1\n",
      "[INFO][2022/01/25 06:10:43 PM] Closing HDF file: /home/nathan/data/ND0011/Pos6/tracks.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO][2022/01/25 06:10:53 PM] Ending BayesianTracker session\n"
     ]
    }
   ],
   "source": [
    "# initialise a tracker session using a context manager\n",
    "with btrack.BayesianTracker() as tracker:\n",
    "\n",
    "    # configure the tracker using a config file\n",
    "    tracker.configure_from_file(\n",
    "        './models/MDCK_config_wildtype.json'\n",
    "    )\n",
    "    tracker.max_search_radius = 40\n",
    "\n",
    "    # append the objects to be tracked\n",
    "    tracker.append(objects)\n",
    "\n",
    "    # set the volume\n",
    "    tracker.volume=((0, 1200), (0, 1600), (-1e5, 1e5))\n",
    "\n",
    "    # track them (in interactive mode)\n",
    "    tracker.track_interactive(step_size=100)\n",
    "\n",
    "    # generate hypotheses and run the global optimizer\n",
    "    tracker.optimize()\n",
    "\n",
    "    tracker.export(f'{root_dir}/{expt}/{pos}/tracks.h5', obj_type='obj_type_1')\n",
    "\n",
    "    # get the tracks in a format for napari visualization (optional)\n",
    "    visaulise_tracks, properties, graph = tracker.to_napari(ndim=2)\n",
    "    \n",
    "    tracks = tracker.tracks"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CellX",
   "language": "python",
   "name": "cellx"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
